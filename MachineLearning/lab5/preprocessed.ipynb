{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab5 数据预处理\n",
    "流程如下：\n",
    "- 数据读取\n",
    "- 去噪（有大量outliers，必须进行）（将离群值设置为```nan```，交由下一步补全数据）\n",
    "- 处理缺失数据\n",
    "- 降维\n",
    "- 标准化\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from funs import *\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from skrebate import ReliefF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据读取\n",
    "X_df = pd.read_csv('Dataset/train_feature.csv')\n",
    "y_df = pd.read_csv('Dataset/train_label.csv')\n",
    "c = y_df['label'].unique().__len__() #分类数\n",
    "\n",
    "X_pred_df = pd.read_csv('Dataset/test_feature.csv')\n",
    "#print(X_pred_df.isnull().sum(), X_df.isnull().sum())\n",
    "\n",
    "X, y, X_pred = X_df.values, y_df.values, X_pred_df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 去噪\n",
    "- 经过观察，离群值过于离谱，应该重做。将各个属性的outliers值设置为nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in X_df.columns:\n",
    "    Q1 = X_df[feature].quantile(0.1)\n",
    "    Q3 = X_df[feature].quantile(0.9)\n",
    "    IQR = Q3 - Q1\n",
    "    X_df.loc[X_df[(X_df[feature]<Q1-1.5*IQR)|(X_df[feature]>Q3+1.5*IQR)].index,[feature]]=np.nan\n",
    "\n",
    "    X_pred_df.loc[X_pred_df[(X_pred_df[feature]<Q1-1.5*IQR)|(X_pred_df[feature]>Q3+1.5*IQR)].index,[feature]]=np.nan\n",
    "\n",
    "X, y, X_pred = X_df.values, y_df.values, X_pred_df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理缺失数据\n",
    "以下方法选择一项：\n",
    "- 平均值填补：在缺失处填上同类在该特征上的均值\n",
    "- KNN方法填补：根据不完整样本距离完整样本的远近，进行拟合，并推算缺失值\n",
    "- 过滤：直接删除不完整的样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理缺失数据：填补（平均值法）\n",
    "for feature in X_df.columns:\n",
    "    mean = X_df[feature].mean()\n",
    "    X_df.loc[X_df[np.isnan(X_df[feature])].index,feature] = mean \n",
    "\n",
    "X, y, X_pred = X_df.values, y_df.values, X_pred_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理缺失数据：填补（KNN方法，注意要先标准化）\n",
    "# 可能是冗余特征的干扰，效果不好\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5,weights='uniform', metric='nan_euclidean')   # 需要归一化（？）\n",
    "X = imputer.fit_transform(X)\n",
    "X_pred = imputer.transform(X_pred)                                              # 对预测目标进行填补\n",
    "#X_imputed = pd.DataFrame(imputed, columns=X.columns)\n",
    "\n",
    "X_df = pd.DataFrame(X, columns=X_df.columns)\n",
    "X_pred_df = pd.DataFrame(X_pred, columns=X_pred_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理缺失数据：过滤（只使用完整数据，丢弃不完整数据）\n",
    "tmp = pd.concat([X_df,y_df],axis=1)\n",
    "tmp = tmp.dropna()#.values\n",
    "y_df = tmp['label']\n",
    "X_df = tmp.drop(columns=['label'])\n",
    "\n",
    "X = X_df.values\n",
    "y = y_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df.to_csv('Dataset/train_f_preprocessed_1.csv',sep=',',header=True, index=False)\n",
    "y_df.to_csv('Dataset/train_l_preprocessed_1.csv',sep=',',header=True, index=False)\n",
    "\n",
    "X[:10,:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 标准化\n",
    "- 均值归零，特征归一"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标准化\n",
    "# sklearn.preprocessing.robust_scale(X, *, axis=0, with_centering=True, with_scaling=True, quantile_range=(25.0, 75.0), copy=True, unit_variance=False)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "standardScaler = StandardScaler() \n",
    "standardScaler.fit(X_df)\n",
    "\n",
    "X = standardScaler.transform(X_df)\n",
    "X_pred = standardScaler.transform(X_pred_df)\n",
    "y = y_df.values\n",
    "\n",
    "X_df = pd.DataFrame(X, columns=X_df.columns)\n",
    "X_pred_df = pd.DataFrame(X_pred, columns=X_pred_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(X, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征选择\n",
    "- 采用Relif-F算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReliefF(discrete_threshold=1, n_jobs=-1, n_neighbors=5)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# relieff\n",
    "r = ReliefF(discrete_threshold=1, n_neighbors=5, n_jobs=-1)\n",
    "r.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X01': ('continuous', 1.0, 0.0, 1.0, 0.386026469542752),\n",
       " 'X02': ('continuous', 1.0, 0.0, 1.0, 0.47612891617110237),\n",
       " 'X03': ('continuous', 3.0, 0.0, 3.0, 1.004820950386549),\n",
       " 'X04': ('continuous', 1.0, 0.0, 1.0, 0.41305192141100605),\n",
       " 'X05': ('continuous', 1.0, 0.0, 1.0, 0.34053532461934904),\n",
       " 'X06': ('continuous', 8.1, 0.015, 8.084999999999999, 0.6104064856533892),\n",
       " 'X07': ('continuous', 41.667, 0.0, 41.667, 2.9238644597700594),\n",
       " 'X08': ('continuous', 7.0, 0.09, 6.91, 0.8442790554809808),\n",
       " 'X09': ('continuous', 1.6, 0.04, 1.56, 0.21513716761930315),\n",
       " 'X10': ('continuous', 1.0, 0.0, 1.0, 0.35303796559921335),\n",
       " 'X11': ('continuous', 1.0, 0.0, 1.0, 0.39342025072640907)}"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00442997, 0.00162866, 0.02442997, 0.00390879, 0.01211726,\n",
       "       0.00319137, 0.00453451, 0.01353852, 0.00678193, 0.10019544,\n",
       "       0.04462541])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 features:  0.8192589630814341\n",
      "2 features:  0.8192589630814341\n",
      "3 features:  0.8192589630814341\n",
      "4 features:  0.8192589630814341\n",
      "5 features:  0.8192589630814341\n",
      "6 features:  0.8143809143009463\n",
      "7 features:  0.8176329468212715\n",
      "8 features:  0.8176329468212715\n",
      "9 features:  0.8176329468212715\n",
      "10 features:  0.8176329468212715\n",
      "11 features:  0.8176329468212715\n"
     ]
    }
   ],
   "source": [
    "# 以决策树模型作为优化对象\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "\n",
    "model_DecTree = DecisionTreeClassifier(criterion = \"entropy\", max_depth=2, splitter='best',min_samples_leaf=5, random_state=0) #调包\n",
    "\n",
    "for n_feature in range(1, 12, 1):\n",
    "    r.set_params(n_features_to_select=n_feature)\n",
    "    X_train = r.transform(X)\n",
    "    # print(X_train.shape)\n",
    "    y_train = y\n",
    "    print(n_feature, \"features: \", np.mean(cross_val_score(model_DecTree, X_train, y_train)))#cv=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 其他数据集测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('E:\\课程资料\\Maths\\应用数学\\机器学习概论\\mylab\\lab1\\loan.csv')\n",
    "df.drop(\"Loan_ID\", axis=1, inplace=True)\n",
    "# df = df.dropna(how='any') # 删除不完整数据，与后面的knn算法二选一\n",
    "\n",
    "df.Gender = df.Gender.map({'Male':1,'Female':0})\n",
    "df.Married = df.Married.map({'Yes':1,'No':0})\n",
    "df.Dependents = df.Dependents.map({'3+':3, '2':2, '1':1, '0':0})\n",
    "df.Education = df.Education.map({'Graduate':1, 'Not Graduate':0})\n",
    "df.Self_Employed = df.Self_Employed.map({'Yes':1, 'No':0})\n",
    "df.Property_Area = df.Property_Area.map({\"Urban\":1, 'Semiurban':0.5, 'Rural':0})\n",
    "df.Loan_Status = df.Loan_Status.map({'Y':1, 'N':0})\n",
    "df.Loan_Amount_Term = df.Loan_Amount_Term / 300\n",
    "df.LoanAmount = df.LoanAmount /100\n",
    "df.ApplicantIncome = df.ApplicantIncome / 10000\n",
    "df.CoapplicantIncome = df.CoapplicantIncome /1000 # 数据预处理，简单约化一下\n",
    "\n",
    "# knn算法\n",
    "from sklearn.impute import KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=5,weights='uniform', metric='nan_euclidean')   # 需要归一化（？）\n",
    "data = imputer.fit_transform(df)\n",
    "df = pd.DataFrame(data, columns=df.columns)\n",
    "\n",
    "df = df.astype(dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        0., 1., 1.]),\n",
       " array([1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        0., 1., 1.]))"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "y = df['Loan_Status'].values\n",
    "X = df.drop(columns=['Loan_Status']).values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)\n",
    "(X[:20, 9]), df[:20]['Credit_History'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练测试\n",
    "- 神经网络，随机森林，lab1逻辑回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8127282420365187\n"
     ]
    }
   ],
   "source": [
    "# lab1逻辑回归 二分类\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression as LR#调包\n",
    "#from funs import LogisticRegression #自己的\n",
    "\n",
    "model_logistic = LR(fit_intercept=True,C=0.2)\n",
    "#model = LogisticRegression(fit_intercept=True,la=1,lr=0.001)\n",
    "\n",
    "print(np.mean(cross_val_score(model_logistic, X, y.reshape(len(y)),cv=5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8041666666666668\n"
     ]
    }
   ],
   "source": [
    "# 神经网络\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from skrebate import ReliefF\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "model_MLP = MLPClassifier(hidden_layer_sizes=[20],max_iter=2000,learning_rate_init=1e-3,tol=1e-4,alpha=3e-6,activation='logistic')\n",
    "\n",
    "clf = model_MLP\n",
    "print(np.mean(cross_val_score(clf, X, y.reshape(len(y)), cv=5)))\n",
    "\n",
    "# 特征选择\n",
    "#for n_feature in range(10, 120,10):#range(1, X.shape[1]+1):\n",
    "#    clf = make_pipeline(ReliefF(n_features_to_select=n_feature, discrete_threshold=5, n_neighbors=5, n_jobs=-1), \n",
    "#        model_MLP)\n",
    "#    print(np.mean(cross_val_score(clf, X, y, cv=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_MLP.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=[10],max_iter=10000,learning_rate_init=1e-3,tol=1e-5,alpha=1e-1,activation='identity')\n",
    "#for i in range(5):\n",
    "# clf.score() accuarcy\n",
    "\n",
    "print(np.mean(cross_val_score(clf, X, y.reshape(len(y)), cv=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 寻找众数\n",
    "for feature in X_df.columns:\n",
    "    tmp = [i for i in X_df[feature]]\n",
    "    n = max(set(tmp),key=tmp.count)\n",
    "    if n==n:# 判断nan\n",
    "        print(max(set(tmp),key=tmp.count))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
